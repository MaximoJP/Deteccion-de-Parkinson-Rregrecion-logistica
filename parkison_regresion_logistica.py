# -*- coding: utf-8 -*-
"""Parkison Regresion logistica.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KkUnuvcm8mvTL3J3LM6qqf_dHy8SnOby
"""

import pandas as pd #paqueterias que nos ayudaron a ver la exploracion de los datos y hacer la prediccion sobre el metodo de Regresion Logistica.
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from collections import Counter
from scipy import stats
from scipy.stats import pearsonr
from sklearn import svm

parkinsson = pd.read_csv('Parkinsson disease.csv')#Mandamos a llamar la base de datos.

parkinsson.head(5) #Visualizamos los datos

datos=parkinsson.select_dtypes(include=['float64','int'])#Selecionamos los datos tipo flotante y entero

par=sns.pairplot(parkinsson)#Vemos las distribuciones de todas las variables con respecto a las demas, de aqui surge la idea de sacar la correlacion y la covarianza.

corre=datos.corr(method='pearson')
corre #aqui guardamos los datos obtenidos de la correlacion con el metodo de Pearson.

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10)) # La Grafica de la matriz de Correlacion donde vemos las variales mas cercanas a uno para poder seleccionar para el metodo 

sns.heatmap(
    corre,
    annot     = True,
    cbar      = False,
    annot_kws = {"size": 8},
    vmin      = -1,
    vmax      = 1,
    center    = 0,
    cmap      = sns.diverging_palette(20, 220, n=200),
    square    = True,
    ax        = ax
)

ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation = 45,
    horizontalalignment = 'right',
)

ax.tick_params(labelsize = 10)

parkinsson.isnull().any() #vemos que no tengamos datos nulos

parkinsson.dtypes #vemos que las todas sean flotantes y enteros donde vemos que la unica que es entereo es el status, que es por la cual vamos a predecir siendo 0 no parkinsson y 1 si parkinsson

cov=parkinsson.cov() #vemos la covarianza de los datos, lo cual es con la que mejor no guiamos al ver que caracteristicas nos aportan mas para la prediccion.
parkinsson.cov()

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15)) # la grafica de la Covarianza 

sns.heatmap(
    cov,
    annot     = True,
    cbar      = False,
    annot_kws = {"size": 8},
    vmin      = -1,
    vmax      = 1,
    center    = 0,
    cmap      = sns.diverging_palette(20, 220, n=200),
    square    = True,
    ax        = ax
)

ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation = 45,
    horizontalalignment = 'right',
)

ax.tick_params(labelsize = 10)

col =['MDVP:Jitter(%)','MDVP:Jitter(Abs)','MDVP:RAP','MDVP:PPQ','Jitter:DDP','MDVP:Shimmer','MDVP:Shimmer(dB)'
          ,'Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ','Shimmer:DDA','NHR','status','RPDE','DFA','spread1','spread2','D2','PPE']#aqui ya seleccionamos las variables que nos sirven para tener una prediccion mejor

carac=parkinsson[col]
carac #las guardamos en carac

def outliersd(df,n,features): #Aqui definimos un funcion para poder descartar algunos datos que esten muy alejados
      outlier_indices = []
      
      # itera sobre las columnas
      for carac in features:
      
          Q1 = np.percentile(df[carac], 25)
        
          Q3 = np.percentile(df[carac],75)
          
          IQR = Q3 - Q1 #IQR Formula
          
          
          outlier_step = 1.5 * IQR
          
          
          outlier_list_col = df[(df[carac] < Q1 - outlier_step) | (df[carac] > Q3 + outlier_step )].index
          
          
          outlier_indices.extend(outlier_list_col)
          
    
      outlier_indices = Counter(outlier_indices)        
      multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )
      
      return multiple_outliers



datadrop=outliersd(parkinsson,2,carac)

datadrop #indices que va a tirar o valores de data frame que no va a tomar

carac.loc[datadrop]#buscamos los datos que queremos tirar 
carac2=carac.drop(datadrop, axis = 0).reset_index(drop=True) #guardamos los nuevos datos 
print(carac2)
len(carac2)

X=carac2
Y=carac2.status
len(X),len(carac) #como vemos len(X)=179 son los datoslimpios con el IQR y len(carc)=195 son los datos orginales.

sns.violinplot(p,Y,data=parkinsson) #Esta es una peque√±a grafica de violin, donde vemos el enfoque de los datos viejos con los datos 'limpios' siendo P=carac y Y los datos limpios sobre la categorica 'status'

X_train,X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.30, random_state=0) #aqui es donde empezamos a tomar la particion de que datos queremos para entrenar que es 70/30

Logreg=LogisticRegression() #Definimos el metodo que queremos realizar para la prediccion que este es Regresion logistica 
Logreg.fit(X_train,Y_train)#Ajustamos los datos 
y_pred=Logreg.predict(X_test) #siendo esta nuestra prediccion con base a la prueba

y_pred #Nuestra prediccion

cnf_matrix=metrics.confusion_matrix(Y_test,y_pred)#hacemos una matriz de confusion para poder ver la eficacia del metodo
cnf_matrix

class_names=[0,1] #hacemos la grafica donde podemos ver que este tiene el 100 de efectividad ya que en 0,0 muestra 14 en 0,1 muestra 0 al igual que en 1,0 y en 11 muestra 40 
fig,ax =plt.subplots()                                           #donde 0,0 son las personas que no tienen parkinsson y 1,1 las personas que tienen parkinsson
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks,class_names)
plt.yticks(tick_marks,class_names)

sns.heatmap(pd.DataFrame(cnf_matrix),annot=True,cmap='Blues_r',fmt='g')
ax.xaxis.set_label_position('top')
plt.tight_layout()
plt.title('Matriz de confucion', y=1.1)
plt.ylabel('Etiqueta actual')
plt.xlabel('Etiqueta de prediccion')

print('Exactitud', metrics.accuracy_score(Y_test,y_pred)) #otro metedo de comprobacion es para ver la exactitud que segun es de 100%

clf=svm.SVC()#realizamos otro metodo para comprobar siendo SVC que es Support vector machine
clf.fit(X,Y)

n_pred=clf.predict(X_test)#vemos la prediccion

n_pred

cnf2_matrix=metrics.confusion_matrix(Y_test,n_pred)
cnf2_matrix #hacemos nuestra matriz y vemos que es igual que nuestra regresion logistica

z=carac #por ultimo hacemos la prueba sin la limpieza de datos (IQR)
p=carac.status

z_train,z_test, p_train, p_test = train_test_split(z,p,test_size=0.30, random_state=0) #tomamos nuestra muestra para trabajar que igual es de 70/30

Logreg=LogisticRegression()#al igual que lo interior hacemos nuestro metodo
Logreg.fit(z_train,p_train)
p_pred=Logreg.predict(z_test)

p_matrix=metrics.confusion_matrix(p_test,p_pred)
p_matrix#vemos que sale aumenta un poco los datos ya que tomamos todos

class_names=[0,1]# De igual mado mostramos la grafica donde vemos que sigue la misma efectividad del 100
fig,ax =plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks,class_names)
plt.yticks(tick_marks,class_names)

sns.heatmap(pd.DataFrame(p_matrix),annot=True,cmap='Blues_r',fmt='g')
ax.xaxis.set_label_position('top')
plt.tight_layout()
plt.title('Matriz de confucion', y=1.1)
plt.ylabel('Etiqueta actual')
plt.xlabel('Etiqueta de prediccion')

print('Exactitud', metrics.accuracy_score(p_test,p_pred))# Veo que esta base de datos nos dice mucho, pero como son muchas 
                                                         #grabaciones de los mismos sujetos en mi opinion creo nodemos predecit mucho.

